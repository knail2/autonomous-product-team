{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44926631",
   "metadata": {},
   "source": [
    "## building GPT from scratch, with notes\n",
    "\n",
    "\n",
    "Andrej Karpathy, a rock star in the world of LLMs, made a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) about a year ago walking through how to build an attention transformer based on the seminal paper \"Attention is all you need\" which was authored in 2017, which really kicked off this whole innovation that has led us to this crazy era of LLMs.\n",
    "\n",
    "The goal of this tutorial is for self learning and understanding how a transformer is built and trained, which will help me fine-tune models and understand the deeper nuances to make the right choices. \n",
    "\n",
    "Also I may just end up using a different data set than Shakespeare text to train this model.\n",
    "\n",
    "Ultimately, the goal is to fine tune a model on a custom code repository, so we will get into fine-tuning algorithms too, like QLORA etc. Anyway, getting ahead of myself here we go.\n",
    "\n",
    "I'll put the time stamp of the video in comments of the code where he says something noteworthy, or sometimes, just for checkposts in this notebook.\n",
    "\n",
    "Omer\n",
    "1.15.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ce4169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:16:16.855579Z",
     "start_time": "2024-01-16T00:16:16.852656Z"
    }
   },
   "outputs": [],
   "source": [
    "# manual step alert! I downloaded, created a text file and cleaned it up a little for the full corpus of Khalil Gibran under ../data/khalil.txt\n",
    "# I downloaded it from https://archive.org/stream/the-complete-works-of-khalil-gibran/The%20complete%20works%20of%20Khalil%20Gibran_djvu.txt\n",
    "# the following downloads the file in html format, yuck\n",
    "#!wget https://archive.org/stream/the-complete-works-of-khalil-gibran/The%20complete%20works%20of%20Khalil%20Gibran_djvu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1d925c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:20:25.438034Z",
     "start_time": "2024-01-16T00:20:25.431378Z"
    }
   },
   "outputs": [],
   "source": [
    "# read it in and inspect\n",
    "with open('../data/khalil.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9693e30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:20:27.790331Z",
     "start_time": "2024-01-16T00:20:27.785618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the data in characters:  914761\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the data in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e83dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:20:29.112589Z",
     "start_time": "2024-01-16T00:20:29.109032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A TEAR AND A SMILE \n",
      "\n",
      "\n",
      "The Creation \n",
      "( = = C) \n",
      "\n",
      "\n",
      "The God separated a spirit from Himself and fashioned it into Beauty. He \n",
      "showered upon her all the blessings of gracefulness and kindness. He gave her \n",
      "the cup of happiness and said, “Drink not from this cup unless you forget the \n",
      "past and the future, for happiness is naught but the moment.” And He also gave \n",
      "her a cup of sorrow and said, “Drink from this cup and you will understand the \n",
      "meaning of the fleeting instants of the joy of life, for sorrow ever abounds.” \n",
      "\n",
      "And the God bestowed upon her a love that would desert he forever upon her \n",
      "first sigh of earthly satisfaction, and a sweetness that would vanish with her first \n",
      "awareness of flattery. \n",
      "\n",
      "And He gave her wisdom from heaven to lead to the all-righteous path, and \n",
      "placed in the depth of her heart and eye that sees the unseen, and created in he an \n",
      "affection and goodness toward all things. He dressed her with raiment of hopes \n",
      "spun by the angels of heaven from the sinews of the \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "552f7012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:24:18.374477Z",
     "start_time": "2024-01-16T00:24:18.352786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'()*+,-.0123459:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWYZ_abcdefghijklmnopqrstuvwxyz|~©»é—‘’“”\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "# identify all unique characters used in this corpus, since we are going to make a character based transformer\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226fe6a",
   "metadata": {},
   "source": [
    "We will now tokenize the text, and essentially here each character is treated as a token. This is to convert alphabets which computers don't understand into numbers which they do. Each token will have an essence and a meaning associated with it, but more on this later.\n",
    "\n",
    "that is different from GPT which used sub-words as tokens. A great explanation of the sub-word token and why it is better than the full word as a token or a character as a token is in [this](https://www.superdatascience.com/podcast/subword-tokenization-with-byte-pair-encoding) short clip by Jon Krohn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca6b7184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T01:33:23.158366Z",
     "start_time": "2024-01-16T01:33:23.152428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 65, 65, 54, 1, 52, 65, 75, 2]\n",
      "good boy!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder takes a string, and outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder takes a list of int and returns a string \n",
    "                                                 #(the join combines the array of strings to make it a string)\n",
    "\n",
    "print(encode(\"good boy!\"))\n",
    "print(decode(encode(\"good boy!\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c0c1d",
   "metadata": {},
   "source": [
    "we just made a character level tokenizer! there are many others. \n",
    "- Google uses [sentencepiece](https://github.com/google/sentencepiece)\n",
    "- OpenAI uses [tiktoken](https://github.com/openai/tiktoken)\n",
    "\n",
    "Both are sub-word tokenizers. e.g. \"related\" is not a token, but \"re\", \"la\" , \"ted\" could be tokens. Yes they make no sense to humans, but when LLMs see these subwords joined with each other, they can derive semantic meaning, e.g. if we prefixed the sub-work \"un\" in front of \"related\" the LLMs would be able to see that unrelated and related are connected, and given the \"essence\" of what the LLM knows about \"un\", it would assume that the full word \"unrelated\" is the opposite of whatever follows \"un\", which is \"related\". This will make more sense when we talk about stage 1 of the tokenizer.\n",
    "\n",
    "As an example let's try openAI's subword tokenizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c63ef60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T01:45:10.379920Z",
     "start_time": "2024-01-16T01:45:10.373918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of all the subtokens in gpt2 are 50257\n",
      "good boy encodes to [11274, 2933, 0] in gpt2\n",
      "11274 decodes to good in gpt2\n",
      "2933 decodes to  boy in gpt2\n",
      "0 decodes to ! in gpt2\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(\"the number of all the subtokens in gpt2 are {}\".format(enc.n_vocab))\n",
    "print(\"good boy encodes to {} in gpt2\".format(enc.encode(\"good boy!\")))\n",
    "print(\"11274 decodes to {} in gpt2\".format(enc.decode([11274])))\n",
    "print(\"2933 decodes to {} in gpt2\".format(enc.decode([2933])))\n",
    "print(\"0 decodes to {} in gpt2\".format(enc.decode([0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d3b5f",
   "metadata": {},
   "source": [
    "We will now encode the entire text data set.. we will use the pytorch library, specifically tensors.\n",
    "tensors are multi-dimensional, highly efficient arrays of the same data type. We can create multi arrays by making arrays within arrays for example, but that is highly inefficient compared to tensor. we will see why **multi-dimensional** is so important soon in the transformer stages.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c80be010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T01:54:26.992264Z",
     "start_time": "2024-01-16T01:54:26.903671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of this encoded tensor with all of the text data is torch.Size([914761])\n",
      "by the way, the shape of the original data was 914761 and since this is character encoding, i.e. 1:1 mapping of the character to the number, this actually makes sense\n",
      "the data type of this encoded tensor is torch.int64\n",
      "here is a sample of the first 50 characters:\n",
      "tensor([25,  1, 44, 29, 25, 42,  1, 25, 38, 28,  1, 25,  1, 43, 37, 33, 36, 29,\n",
      "         1,  0,  0,  0, 44, 58, 55,  1, 27, 68, 55, 51, 70, 59, 65, 64,  1,  0,\n",
      "         4,  1, 21,  1, 21,  1, 27,  5,  1,  0,  0,  0, 44, 58])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"the shape of this encoded tensor with all of the text data is {}\".format(data.shape))\n",
    "print(\"by the way, the shape of the original data was {} and since this is character encoding, i.e.\\\n",
    " 1:1 mapping of the character to the number, this actually makes sense\".format(len(text)))\n",
    "print(\"the data type of this encoded tensor is {}\".format(data.dtype))\n",
    "print(\"here is a sample of the first 50 characters:\\n{}\".format(data[:50]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
