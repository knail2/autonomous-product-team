{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44926631",
   "metadata": {},
   "source": [
    "## building GPT from scratch, with notes\n",
    "\n",
    "\n",
    "Andrej Karpathy, a rock star in the world of LLMs, made a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) about a year ago walking through how to build an attention transformer based on the seminal paper \"Attention is all you need\" which was authored in 2017, which really kicked off this whole innovation that has led us to this crazy era of LLMs.\n",
    "\n",
    "The goal of this tutorial is for self learning and understanding how a transformer is built and trained, which will help me fine-tune models and understand the deeper nuances to make the right choices. \n",
    "\n",
    "Also I may just end up using a different data set than Shakespeare text to train this model.\n",
    "\n",
    "Ultimately, the goal is to fine tune a model on a custom code repository, so we will get into fine-tuning algorithms too, like QLORA etc. Anyway, getting ahead of myself here we go.\n",
    "\n",
    "I'll put the time stamp of the video in comments of the code where he says something noteworthy, or sometimes, just for checkposts in this notebook.\n",
    "\n",
    "Omer\n",
    "1.15.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ce4169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:16:16.855579Z",
     "start_time": "2024-01-16T00:16:16.852656Z"
    }
   },
   "outputs": [],
   "source": [
    "# manual step alert! I downloaded, created a text file and cleaned it up a little for the full corpus of Khalil Gibran under ../data/khalil.txt\n",
    "# I downloaded it from https://archive.org/stream/the-complete-works-of-khalil-gibran/The%20complete%20works%20of%20Khalil%20Gibran_djvu.txt\n",
    "# the following downloads the file in html format, yuck\n",
    "#!wget https://archive.org/stream/the-complete-works-of-khalil-gibran/The%20complete%20works%20of%20Khalil%20Gibran_djvu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1d925c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:20:25.438034Z",
     "start_time": "2024-01-16T00:20:25.431378Z"
    }
   },
   "outputs": [],
   "source": [
    "# read it in and inspect\n",
    "with open('../data/khalil.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9693e30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:20:27.790331Z",
     "start_time": "2024-01-16T00:20:27.785618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the data in characters:  914761\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the data in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e83dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:20:29.112589Z",
     "start_time": "2024-01-16T00:20:29.109032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A TEAR AND A SMILE \n",
      "\n",
      "\n",
      "The Creation \n",
      "( = = C) \n",
      "\n",
      "\n",
      "The God separated a spirit from Himself and fashioned it into Beauty. He \n",
      "showered upon her all the blessings of gracefulness and kindness. He gave her \n",
      "the cup of happiness and said, “Drink not from this cup unless you forget the \n",
      "past and the future, for happiness is naught but the moment.” And He also gave \n",
      "her a cup of sorrow and said, “Drink from this cup and you will understand the \n",
      "meaning of the fleeting instants of the joy of life, for sorrow ever abounds.” \n",
      "\n",
      "And the God bestowed upon her a love that would desert he forever upon her \n",
      "first sigh of earthly satisfaction, and a sweetness that would vanish with her first \n",
      "awareness of flattery. \n",
      "\n",
      "And He gave her wisdom from heaven to lead to the all-righteous path, and \n",
      "placed in the depth of her heart and eye that sees the unseen, and created in he an \n",
      "affection and goodness toward all things. He dressed her with raiment of hopes \n",
      "spun by the angels of heaven from the sinews of the \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8bb1c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T00:24:18.374477Z",
     "start_time": "2024-01-16T00:24:18.352786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'()*+,-.0123459:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWYZ_abcdefghijklmnopqrstuvwxyz|~©»é—‘’“”\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "# identify all unique characters used in this corpus, since we are going to make a character based transformer\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58107e",
   "metadata": {},
   "source": [
    "We will now tokenize the text, and essentially here each character is treated as a token. This is to convert alphabets which computers don't understand into numbers which they do. Each token will have an essence and a meaning associated with it, but more on this later.\n",
    "\n",
    "that is different from GPT which used sub-words as tokens. A great explanation of the sub-word token and why it is better than the full word as a token or a character as a token is in [this](https://www.superdatascience.com/podcast/subword-tokenization-with-byte-pair-encoding) short clip by Jon Krohn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6464a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T01:33:23.158366Z",
     "start_time": "2024-01-16T01:33:23.152428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 65, 65, 54, 1, 52, 65, 75, 2]\n",
      "good boy!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder takes a string, and outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder takes a list of int and returns a string \n",
    "                                                 #(the join combines the array of strings to make it a string)\n",
    "\n",
    "print(encode(\"good boy!\"))\n",
    "print(decode(encode(\"good boy!\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5dc18",
   "metadata": {},
   "source": [
    "we just made a character level tokenizer! there are many others. \n",
    "- Google uses [sentencepiece](https://github.com/google/sentencepiece)\n",
    "- OpenAI uses [tiktoken](https://github.com/openai/tiktoken)\n",
    "\n",
    "Both are sub-word tokenizers. e.g. \"related\" is not a token, but \"re\", \"la\" , \"ted\" could be tokens. Yes they make no sense to humans, but when LLMs see these subwords joined with each other, they can derive semantic meaning, e.g. if we prefixed the sub-work \"un\" in front of \"related\" the LLMs would be able to see that unrelated and related are connected, and given the \"essence\" of what the LLM knows about \"un\", it would assume that the full word \"unrelated\" is the opposite of whatever follows \"un\", which is \"related\". This will make more sense when we talk about stage 1 of the tokenizer.\n",
    "\n",
    "As an example let's try openAI's subword tokenizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef520efa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T01:45:10.379920Z",
     "start_time": "2024-01-16T01:45:10.373918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of all the subtokens in gpt2 are 50257\n",
      "good boy encodes to [11274, 2933, 0] in gpt2\n",
      "11274 decodes to good in gpt2\n",
      "2933 decodes to  boy in gpt2\n",
      "0 decodes to ! in gpt2\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(\"the number of all the subtokens in gpt2 are {}\".format(enc.n_vocab))\n",
    "print(\"good boy encodes to {} in gpt2\".format(enc.encode(\"good boy!\")))\n",
    "print(\"11274 decodes to {} in gpt2\".format(enc.decode([11274])))\n",
    "print(\"2933 decodes to {} in gpt2\".format(enc.decode([2933])))\n",
    "print(\"0 decodes to {} in gpt2\".format(enc.decode([0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adea1a0",
   "metadata": {},
   "source": [
    "We will now encode the entire text data set.. we will use the pytorch library, specifically tensors.\n",
    "tensors are multi-dimensional, highly efficient arrays of the same data type. We can create multi arrays by making arrays within arrays for example, but that is highly inefficient compared to tensor. we will see why **multi-dimensional** is so important soon in the transformer stages.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2dd0f64e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T01:54:26.992264Z",
     "start_time": "2024-01-16T01:54:26.903671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of this encoded tensor with all of the text data is torch.Size([914761])\n",
      "by the way, the shape of the original data was 914761 and since this is character encoding, i.e. 1:1 mapping of the character to the number, this actually makes sense\n",
      "the data type of this encoded tensor is torch.int64\n",
      "here is a sample of the first 50 characters:\n",
      "tensor([25,  1, 44, 29, 25, 42,  1, 25, 38, 28,  1, 25,  1, 43, 37, 33, 36, 29,\n",
      "         1,  0,  0,  0, 44, 58, 55,  1, 27, 68, 55, 51, 70, 59, 65, 64,  1,  0,\n",
      "         4,  1, 21,  1, 21,  1, 27,  5,  1,  0,  0,  0, 44, 58])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"the shape of this encoded tensor with all of the text data is {}\".format(data.shape))\n",
    "print(\"by the way, the shape of the original data was {} and since this is character encoding, i.e.\\\n",
    " 1:1 mapping of the character to the number, this actually makes sense\".format(len(text)))\n",
    "print(\"the data type of this encoded tensor is {}\".format(data.dtype))\n",
    "print(\"here is a sample of the first 50 characters:\\n{}\".format(data[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5a016",
   "metadata": {},
   "source": [
    "We will now split this model into training and test data, so we can check out (aka validate) how close it the our vocabulary to Khalil Gibran's style using the test data.\n",
    "\n",
    "This is a standard concept in data science. if you want to learn more, try [here](https://www.obviously.ai/post/the-difference-between-training-data-vs-test-data-in-machine-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bad644f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T02:02:22.424011Z",
     "start_time": "2024-01-16T02:02:22.418743Z"
    }
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # n will be 90% of the (character) length of all the data, so 90% of 914761\n",
    "train_data = data[:n] # train our model Bon first 90% of the length of data\n",
    "val_data = data[n:] # we will validate on the last 90% on how accurate our model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7728c",
   "metadata": {},
   "source": [
    "We will now send \"chunks\" of the data in the dataset to the model to train it. can't send it all of the data, as it would be very computationally hard to handle. so we train on *randomly sampled* chunks.\n",
    "\n",
    "These chunks have a maximum length (which will make sense why b/c stage 1 of the transformer is limited by the how many tokens can be sent to it in parallel. We will call this block_size. (It can also be called context length in terms of the input tokens the gpt can accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfd8c596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T02:09:31.669269Z",
     "start_time": "2024-01-16T02:09:31.664351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25,  1, 44, 29, 25, 42,  1, 25, 38])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "# we will send the following chunk for training..\n",
    "print(train_data[:block_size+1]) # notice how we are sending 9 characters, not 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dbef802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T02:10:04.600871Z",
     "start_time": "2024-01-16T02:10:04.596395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A TEAR AN\n"
     ]
    }
   ],
   "source": [
    "#in the actual data this looks like this:\n",
    "print(text[:block_size+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e662ffc",
   "metadata": {},
   "source": [
    "**Important note!** when we send 9 characters, each of them have some information about the relationship to each other packed into them, e.g.\n",
    "- in the context of A, a space likely comes next.\n",
    "- In the context of \"A \", T likely comes next, \n",
    "- if it is \"A T\" then \"E\" will likely follow, and so on\n",
    "\n",
    "This is why the 9 pieces of data sent will show \"8 relationships\". The 8th example is:\n",
    "- If the 8th phrase \"A TEAR A\" comes, then what follows is likely N.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "623e761a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T02:16:39.876333Z",
     "start_time": "2024-01-16T02:16:39.868028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([25]), the target: 1\n",
      "when input is tensor([25,  1]), the target: 44\n",
      "when input is tensor([25,  1, 44]), the target: 29\n",
      "when input is tensor([25,  1, 44, 29]), the target: 25\n",
      "when input is tensor([25,  1, 44, 29, 25]), the target: 42\n",
      "when input is tensor([25,  1, 44, 29, 25, 42]), the target: 1\n",
      "when input is tensor([25,  1, 44, 29, 25, 42,  1]), the target: 25\n",
      "when input is tensor([25,  1, 44, 29, 25, 42,  1, 25]), the target: 38\n",
      "see! 8 relationships!\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, the target: {target}\")\n",
    "    \n",
    "print(\"see! 8 relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd56d4",
   "metadata": {},
   "source": [
    "So, everytime we send this data into the transformer to train it, we will sample and send many such batches randomly from different location of the corpus\n",
    "\n",
    "We will be sending **many batches all stacked up in a single tensor**, sent to the Xformer. And we do this just for efficiency to keep the GPUs busy as they are very good at parallel processing of data.\n",
    "\n",
    "So while we may be processing these multiple RANDOMLY sampled chunks in parallel in real time, these chunks are processed completely indepdently, they don't talk to each other. \n",
    "\n",
    "tidbit. this is what makes the Transfomer model different from the tradition neural network approaches like LSTM (long short term memory) which send data in sequentially. The folks at openAI when they wrote the paper wanted to optimize for speed and scale, and that is why they didn't use traditional recurrent neural network approaches.\n",
    "\n",
    "Ofcourse, there was a problem with this..when you send data in randomly, the transformer would have to figure out the inter-relationship across these chunks to have complete context. it does that in one of the stages. more to come on that later.\n",
    "\n",
    "\n",
    "now, we will generalize the prior (serial) data chunks and introduce the **batch dimension** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a12143d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T03:02:08.905261Z",
     "start_time": "2024-01-16T03:02:08.890939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[55, 68, 23, 84,  1, 33, 64,  1],\n",
      "        [69, 69,  1, 53, 68, 71, 55, 62],\n",
      "        [55,  1, 65, 64, 55,  1, 73, 59],\n",
      "        [75, 65, 71, 68,  1,  0, 66, 55]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[68, 23, 84,  1, 33, 64,  1, 68],\n",
      "        [69,  1, 53, 68, 71, 55, 62,  1],\n",
      "        [ 1, 65, 64, 55,  1, 73, 59, 70],\n",
      "        [65, 71, 68,  1,  0, 66, 55, 68]])\n",
      "----\n",
      "when input is [55] the target: 68\n",
      "when input is [55, 68] the target: 23\n",
      "when input is [55, 68, 23] the target: 84\n",
      "when input is [55, 68, 23, 84] the target: 1\n",
      "when input is [55, 68, 23, 84, 1] the target: 33\n",
      "when input is [55, 68, 23, 84, 1, 33] the target: 64\n",
      "when input is [55, 68, 23, 84, 1, 33, 64] the target: 1\n",
      "when input is [55, 68, 23, 84, 1, 33, 64, 1] the target: 68\n",
      "when input is [69] the target: 69\n",
      "when input is [69, 69] the target: 1\n",
      "when input is [69, 69, 1] the target: 53\n",
      "when input is [69, 69, 1, 53] the target: 68\n",
      "when input is [69, 69, 1, 53, 68] the target: 71\n",
      "when input is [69, 69, 1, 53, 68, 71] the target: 55\n",
      "when input is [69, 69, 1, 53, 68, 71, 55] the target: 62\n",
      "when input is [69, 69, 1, 53, 68, 71, 55, 62] the target: 1\n",
      "when input is [55] the target: 1\n",
      "when input is [55, 1] the target: 65\n",
      "when input is [55, 1, 65] the target: 64\n",
      "when input is [55, 1, 65, 64] the target: 55\n",
      "when input is [55, 1, 65, 64, 55] the target: 1\n",
      "when input is [55, 1, 65, 64, 55, 1] the target: 73\n",
      "when input is [55, 1, 65, 64, 55, 1, 73] the target: 59\n",
      "when input is [55, 1, 65, 64, 55, 1, 73, 59] the target: 70\n",
      "when input is [75] the target: 65\n",
      "when input is [75, 65] the target: 71\n",
      "when input is [75, 65, 71] the target: 68\n",
      "when input is [75, 65, 71, 68] the target: 1\n",
      "when input is [75, 65, 71, 68, 1] the target: 0\n",
      "when input is [75, 65, 71, 68, 1, 0] the target: 66\n",
      "when input is [75, 65, 71, 68, 1, 0, 66] the target: 55\n",
      "when input is [75, 65, 71, 68, 1, 0, 66, 55] the target: 68\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337) # to make this deterministic for each \"random\" run\n",
    "batch_size = 4 # how many independent sequences will we process in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions.\n",
    "\n",
    "def get_batch(split):\n",
    "    #generates a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "        # doc: https://pytorch.org/docs/stable/generated/torch.randint.html\n",
    "        # torch.randint(low=0, high, size, ...)\n",
    "        # explanation of the prior line of code:\n",
    "        # (batch_size, )  --- (A)\n",
    "        # this determines the number of random numbers generated\n",
    "        # in this case it will be a list of 4 that gets spit out and stored in ix.\n",
    "        # these 4 are selected as the next 4 items after the random starting point\n",
    "        # that random starting point is calculated by:\n",
    "        # len(data) - block_size --- (B)\n",
    "        # So, this is first argument passed to the random integer (randint) function\n",
    "        # this argument basically tells the function, hey find the POSITION of the character data between\n",
    "        # zero and this number. \n",
    "        # Note how it is total length - block_size, imagine if the full length of data was 38\n",
    "        # the sample of the POSITION of the data would not be more than 38-8 = 30\n",
    "        # this is because in the subsequent steps we will be extracting the value of the 30th item \n",
    "        # and the 31st item and the 32nd item all the way to the 8th item \n",
    "        # because we have a batch_size of 8. suppose we didn't have the -block_size part in there\n",
    "        # we could then randomly pick 35 or 36 or even 38, and that would screw up the\n",
    "        # subsequent step, where it would seek the next 8 values, but there wouldn't be the full 8\n",
    "        # and it would error out.\n",
    "        # \n",
    "        #\n",
    "        # so to bring it together (B) identifies the random position of anywhere in the data set.\n",
    "        # and (A) selects four such POSITIONS and return those back out and stores it in ix.\n",
    "        \n",
    "        \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "        # so what that does is it takes the 4 positions identifies in ix earlier, \n",
    "        # pulls the next block_size(8) and stack them up as ROWs in the tensor, \n",
    "        # so it would be an 4x8 (rows x columns) tensor\n",
    "    \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "        # it shows the stack x by an offset of 1..\n",
    "    return x,y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c129f07",
   "metadata": {},
   "source": [
    "So, I want to talk about why we printed the \"y\". Well this is going to be the loss function for our neural network.\n",
    "take an example of the first row printed:\n",
    "\n",
    "- x -> [55, 68, 23, 84,  1, 33, 64,  1]\n",
    "- y -> [68, 23, 84,  1, 33, 64,  1, 68]\n",
    "\n",
    "- if the input is 55, then the desired output is 68 (value in y in the SAME index as the x[])\n",
    "- if the input is 55, 68 then the desired output is 23 (value in y in the SAME index as the max value of x[])\n",
    "\n",
    "\n",
    "this loss function is applied on a neural network all the way in the end to measure what the NN spits out against what should have been the correct answer, and the delta between the NN output and the actual is the \"error\". This \"error\" is backpropagated through the neural network layers to adjust their weights so that the error is minimized.\n",
    "\n",
    "here is more details on the [loss function](https://www.analyticsvidhya.com/blog/2022/06/understanding-loss-function-in-deep-learning/) in ML\n",
    "\n",
    "\n",
    "Coming back to the above example, we have 32 values in x, and 32 values in y (desired targets). essentially we have 32 relationships stored in x and y\n",
    "\n",
    "repeating what we said earlier:\n",
    "\n",
    "- if the input is 55, then the desired output is 68 (value in y in the SAME index as the x[])\n",
    "- if the input is 55, 68 then the desired output is 23 (value in y in the SAME index as the max value of x[])\n",
    "- and so on.\n",
    "\n",
    "So this tensor below:\n",
    "xb tensor([[55, 68, 23, 84,  1, 33, 64,  1],\n",
    "        [69, 69,  1, 53, 68, 71, 55, 62],\n",
    "        [55,  1, 65, 64, 55,  1, 73, 59],\n",
    "        [75, 65, 71, 68,  1,  0, 66, 55]])\n",
    "        \n",
    "will feed into the transformer, and the transformer will simultaneously process these batches. and then look up the correct integers to predict in the same positions in the other tensor:\n",
    "\n",
    "\n",
    "yb tensor([[68, 23, 84,  1, 33, 64,  1, 68],\n",
    "        [69,  1, 53, 68, 71, 55, 62,  1],\n",
    "        [ 1, 65, 64, 55,  1, 73, 59, 70],\n",
    "        [65, 71, 68,  1,  0, 66, 55, 68]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df644b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # stopping at 22:30, where he starts to feed the xb tensor to the bigram language model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
