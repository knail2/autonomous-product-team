{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44926631",
   "metadata": {},
   "source": [
    "## building GPT from scratch, with notes\n",
    "\n",
    "\n",
    "Andrej Karpathy, a rock star in the world of LLMs, made a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) about a year ago walking through how to build an attention transformer based on the seminal paper \"Attention is all you need\" which was authored in 2017, which really kicked off this whole innovation that has led us to this crazy era of LLMs.\n",
    "\n",
    "The goal of this tutorial is for self learning and understanding how a transformer is built and trained, which will help me fine-tune models and understand the deeper nuances to make the right choices. \n",
    "\n",
    "Also I may just end up using a different data set than Shakespeare text to train this model.\n",
    "\n",
    "Ultimately, the goal is to fine tune a model on a custom code repository, so we will get into fine-tuning algorithms too, like QLORA etc. Anyway, getting ahead of myself here we go.\n",
    "\n",
    "I'll put the time stamp of the video in comments of the code where he says something noteworthy, or sometimes, just for checkposts in this notebook.\n",
    "\n",
    "Omer\n",
    "1.15.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ce4169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:24.690248Z",
     "start_time": "2024-01-17T15:40:24.688522Z"
    }
   },
   "outputs": [],
   "source": [
    "# manual step alert! I downloaded, created a text file and cleaned it up a little for the full corpus of Khalil Gibran under ../data/khalil.txt\n",
    "# I downloaded it from https://archive.org/stream/the-complete-works-of-khalil-gibran/The%20complete%20works%20of%20Khalil%20Gibran_djvu.txt\n",
    "# the following downloads the file in html format, yuck\n",
    "#!wget https://archive.org/stream/the-complete-works-of-khalil-gibran/The%20complete%20works%20of%20Khalil%20Gibran_djvu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1d925c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:24.693582Z",
     "start_time": "2024-01-17T15:40:24.691223Z"
    }
   },
   "outputs": [],
   "source": [
    "# read it in and inspect\n",
    "with open('../data/khalil.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9693e30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:24.697243Z",
     "start_time": "2024-01-17T15:40:24.695317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the data in characters:  914761\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the data in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06e83dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:24.699680Z",
     "start_time": "2024-01-17T15:40:24.697921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A TEAR AND A SMILE \n",
      "\n",
      "\n",
      "The Creation \n",
      "( = = C) \n",
      "\n",
      "\n",
      "The God separated a spirit from Himself and fashioned it into Beauty. He \n",
      "showered upon her all the blessings of gracefulness and kindness. He gave her \n",
      "the cup of happiness and said, “Drink not from this cup unless you forget the \n",
      "past and the future, for happiness is naught but the moment.” And He also gave \n",
      "her a cup of sorrow and said, “Drink from this cup and you will understand the \n",
      "meaning of the fleeting instants of the joy of life, for sorrow ever abounds.” \n",
      "\n",
      "And the God bestowed upon her a love that would desert he forever upon her \n",
      "first sigh of earthly satisfaction, and a sweetness that would vanish with her first \n",
      "awareness of flattery. \n",
      "\n",
      "And He gave her wisdom from heaven to lead to the all-righteous path, and \n",
      "placed in the depth of her heart and eye that sees the unseen, and created in he an \n",
      "affection and goodness toward all things. He dressed her with raiment of hopes \n",
      "spun by the angels of heaven from the sinews of the \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8bb1c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:24.708083Z",
     "start_time": "2024-01-17T15:40:24.700375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'()*+,-.0123459:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWYZ_abcdefghijklmnopqrstuvwxyz|~©»é—‘’“”\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "# identify all unique characters used in this corpus, since we are going to make a character based transformer\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58107e",
   "metadata": {},
   "source": [
    "We will now tokenize the text, and essentially here each character is treated as a token. This is to convert alphabets which computers don't understand into numbers which they do. Each token will have an essence and a meaning associated with it, but more on this later.\n",
    "\n",
    "that is different from GPT which used sub-words as tokens. A great explanation of the sub-word token and why it is better than the full word as a token or a character as a token is in [this](https://www.superdatascience.com/podcast/subword-tokenization-with-byte-pair-encoding) short clip by Jon Krohn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6464a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:24.710859Z",
     "start_time": "2024-01-17T15:40:24.708689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 65, 65, 54, 1, 52, 65, 75, 2]\n",
      "good boy!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder takes a string, and outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder takes a list of int and returns a string \n",
    "                                                 #(the join combines the array of strings to make it a string)\n",
    "\n",
    "print(encode(\"good boy!\"))\n",
    "print(decode(encode(\"good boy!\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5dc18",
   "metadata": {},
   "source": [
    "we just made a character level tokenizer! there are many others. \n",
    "- Google uses [sentencepiece](https://github.com/google/sentencepiece)\n",
    "- OpenAI uses [tiktoken](https://github.com/openai/tiktoken)\n",
    "\n",
    "Both are sub-word tokenizers. e.g. \"related\" is not a token, but \"re\", \"la\" , \"ted\" could be tokens. Yes they make no sense to humans, but when LLMs see these subwords joined with each other, they can derive semantic meaning, e.g. if we prefixed the sub-work \"un\" in front of \"related\" the LLMs would be able to see that unrelated and related are connected, and given the \"essence\" of what the LLM knows about \"un\", it would assume that the full word \"unrelated\" is the opposite of whatever follows \"un\", which is \"related\". This will make more sense when we talk about stage 1 of the tokenizer.\n",
    "\n",
    "As an example let's try openAI's subword tokenizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef520efa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:24.866734Z",
     "start_time": "2024-01-17T15:40:24.711577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of all the subtokens in gpt2 are 50257\n",
      "good boy encodes to [11274, 2933, 0] in gpt2\n",
      "11274 decodes to good in gpt2\n",
      "2933 decodes to  boy in gpt2\n",
      "0 decodes to ! in gpt2\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(\"the number of all the subtokens in gpt2 are {}\".format(enc.n_vocab))\n",
    "print(\"good boy encodes to {} in gpt2\".format(enc.encode(\"good boy!\")))\n",
    "print(\"11274 decodes to {} in gpt2\".format(enc.decode([11274])))\n",
    "print(\"2933 decodes to {} in gpt2\".format(enc.decode([2933])))\n",
    "print(\"0 decodes to {} in gpt2\".format(enc.decode([0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adea1a0",
   "metadata": {},
   "source": [
    "We will now encode the entire text data set.. we will use the pytorch library, specifically tensors.\n",
    "tensors are multi-dimensional, highly efficient arrays of the same data type. We can create multi arrays by making arrays within arrays for example, but that is highly inefficient compared to tensor. we will see why **multi-dimensional** is so important soon in the transformer stages.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd0f64e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:25.564552Z",
     "start_time": "2024-01-17T15:40:24.867500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of this encoded tensor with all of the text data is torch.Size([914761])\n",
      "by the way, the shape of the original data was 914761 and since this is character encoding, i.e. 1:1 mapping of the character to the number, this actually makes sense\n",
      "the data type of this encoded tensor is torch.int64\n",
      "here is a sample of the first 50 characters:\n",
      "tensor([25,  1, 44, 29, 25, 42,  1, 25, 38, 28,  1, 25,  1, 43, 37, 33, 36, 29,\n",
      "         1,  0,  0,  0, 44, 58, 55,  1, 27, 68, 55, 51, 70, 59, 65, 64,  1,  0,\n",
      "         4,  1, 21,  1, 21,  1, 27,  5,  1,  0,  0,  0, 44, 58])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(\"the shape of this encoded tensor with all of the text data is {}\".format(data.shape))\n",
    "print(\"by the way, the shape of the original data was {} and since this is character encoding, i.e.\\\n",
    " 1:1 mapping of the character to the number, this actually makes sense\".format(len(text)))\n",
    "print(\"the data type of this encoded tensor is {}\".format(data.dtype))\n",
    "print(\"here is a sample of the first 50 characters:\\n{}\".format(data[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5a016",
   "metadata": {},
   "source": [
    "We will now split this model into training and test data, so we can check out (aka validate) how close it the our vocabulary to Khalil Gibran's style using the test data.\n",
    "\n",
    "This is a standard concept in data science. if you want to learn more, try [here](https://www.obviously.ai/post/the-difference-between-training-data-vs-test-data-in-machine-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad644f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:25.566851Z",
     "start_time": "2024-01-17T15:40:25.565375Z"
    }
   },
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # n will be 90% of the (character) length of all the data, so 90% of 914761\n",
    "train_data = data[:n] # train our model Bon first 90% of the length of data\n",
    "val_data = data[n:] # we will validate on the last 90% on how accurate our model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7728c",
   "metadata": {},
   "source": [
    "We will now send \"chunks\" of the data in the dataset to the model to train it. can't send it all of the data, as it would be very computationally hard to handle. so we train on *randomly sampled* chunks.\n",
    "\n",
    "These chunks have a maximum length (which will make sense why b/c stage 1 of the transformer is limited by the how many tokens can be sent to it in parallel. We will call this block_size. (It can also be called context length in terms of the input tokens the gpt can accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd8c596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:40:55.169412Z",
     "start_time": "2024-01-17T15:40:55.163391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25,  1, 44, 29, 25, 42,  1, 25, 38])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "# we will send the following chunk for training..\n",
    "print(train_data[:block_size+1]) # notice how we are sending 9 characters, not 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dbef802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:41:00.462832Z",
     "start_time": "2024-01-17T15:41:00.458025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A TEAR AN\n"
     ]
    }
   ],
   "source": [
    "#in the actual data this looks like this:\n",
    "print(text[:block_size+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e662ffc",
   "metadata": {},
   "source": [
    "**Important note!** when we send 9 characters, each of them have some information about the relationship to each other packed into them, e.g.\n",
    "- in the context of A, a space likely comes next.\n",
    "- In the context of \"A \", T likely comes next, \n",
    "- if it is \"A T\" then \"E\" will likely follow, and so on\n",
    "\n",
    "This is why the 9 pieces of data sent will show \"8 relationships\". The 8th example is:\n",
    "- If the 8th phrase \"A TEAR A\" comes, then what follows is likely N.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623e761a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:41:04.983790Z",
     "start_time": "2024-01-17T15:41:04.975627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([25]), the target: 1\n",
      "when input is tensor([25,  1]), the target: 44\n",
      "when input is tensor([25,  1, 44]), the target: 29\n",
      "when input is tensor([25,  1, 44, 29]), the target: 25\n",
      "when input is tensor([25,  1, 44, 29, 25]), the target: 42\n",
      "when input is tensor([25,  1, 44, 29, 25, 42]), the target: 1\n",
      "when input is tensor([25,  1, 44, 29, 25, 42,  1]), the target: 25\n",
      "when input is tensor([25,  1, 44, 29, 25, 42,  1, 25]), the target: 38\n",
      "see! 8 relationships!\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, the target: {target}\")\n",
    "    \n",
    "print(\"see! 8 relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd56d4",
   "metadata": {},
   "source": [
    "So, everytime we send this data into the transformer to train it, we will sample and send many such batches randomly from different location of the corpus\n",
    "\n",
    "We will be sending **many batches all stacked up in a single tensor**, sent to the Xformer. And we do this just for efficiency to keep the GPUs busy as they are very good at parallel processing of data.\n",
    "\n",
    "So while we may be processing these multiple RANDOMLY sampled chunks in parallel in real time, these chunks are processed completely indepdently, they don't talk to each other. \n",
    "\n",
    "tidbit. this is what makes the Transfomer model different from the tradition neural network approaches like LSTM (long short term memory) which send data in sequentially. The folks at openAI when they wrote the paper wanted to optimize for speed and scale, and that is why they didn't use traditional recurrent neural network approaches.\n",
    "\n",
    "Ofcourse, there was a problem with this..when you send data in randomly, the transformer would have to figure out the inter-relationship across these chunks to have complete context. it does that in one of the stages. more to come on that later.\n",
    "\n",
    "\n",
    "now, we will generalize the prior (serial) data chunks and introduce the **batch dimension** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a12143d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:41:08.308833Z",
     "start_time": "2024-01-17T15:41:08.292585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[55, 68, 23, 84,  1, 33, 64,  1],\n",
      "        [69, 69,  1, 53, 68, 71, 55, 62],\n",
      "        [55,  1, 65, 64, 55,  1, 73, 59],\n",
      "        [75, 65, 71, 68,  1,  0, 66, 55]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[68, 23, 84,  1, 33, 64,  1, 68],\n",
      "        [69,  1, 53, 68, 71, 55, 62,  1],\n",
      "        [ 1, 65, 64, 55,  1, 73, 59, 70],\n",
      "        [65, 71, 68,  1,  0, 66, 55, 68]])\n",
      "----\n",
      "when input is [55] the target: 68\n",
      "when input is [55, 68] the target: 23\n",
      "when input is [55, 68, 23] the target: 84\n",
      "when input is [55, 68, 23, 84] the target: 1\n",
      "when input is [55, 68, 23, 84, 1] the target: 33\n",
      "when input is [55, 68, 23, 84, 1, 33] the target: 64\n",
      "when input is [55, 68, 23, 84, 1, 33, 64] the target: 1\n",
      "when input is [55, 68, 23, 84, 1, 33, 64, 1] the target: 68\n",
      "when input is [69] the target: 69\n",
      "when input is [69, 69] the target: 1\n",
      "when input is [69, 69, 1] the target: 53\n",
      "when input is [69, 69, 1, 53] the target: 68\n",
      "when input is [69, 69, 1, 53, 68] the target: 71\n",
      "when input is [69, 69, 1, 53, 68, 71] the target: 55\n",
      "when input is [69, 69, 1, 53, 68, 71, 55] the target: 62\n",
      "when input is [69, 69, 1, 53, 68, 71, 55, 62] the target: 1\n",
      "when input is [55] the target: 1\n",
      "when input is [55, 1] the target: 65\n",
      "when input is [55, 1, 65] the target: 64\n",
      "when input is [55, 1, 65, 64] the target: 55\n",
      "when input is [55, 1, 65, 64, 55] the target: 1\n",
      "when input is [55, 1, 65, 64, 55, 1] the target: 73\n",
      "when input is [55, 1, 65, 64, 55, 1, 73] the target: 59\n",
      "when input is [55, 1, 65, 64, 55, 1, 73, 59] the target: 70\n",
      "when input is [75] the target: 65\n",
      "when input is [75, 65] the target: 71\n",
      "when input is [75, 65, 71] the target: 68\n",
      "when input is [75, 65, 71, 68] the target: 1\n",
      "when input is [75, 65, 71, 68, 1] the target: 0\n",
      "when input is [75, 65, 71, 68, 1, 0] the target: 66\n",
      "when input is [75, 65, 71, 68, 1, 0, 66] the target: 55\n",
      "when input is [75, 65, 71, 68, 1, 0, 66, 55] the target: 68\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337) # to make this deterministic for each \"random\" run\n",
    "batch_size = 4 # how many independent sequences will we process in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions.\n",
    "\n",
    "def get_batch(split):\n",
    "    #generates a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "        # doc: https://pytorch.org/docs/stable/generated/torch.randint.html\n",
    "        # torch.randint(low=0, high, size, ...)\n",
    "        # explanation of the prior line of code:\n",
    "        # (batch_size, )  --- (A)\n",
    "        # this determines the number of random numbers generated\n",
    "        # in this case it will be a list of 4 that gets spit out and stored in ix.\n",
    "        # these 4 are selected as the next 4 items after the random starting point\n",
    "        # that random starting point is calculated by:\n",
    "        # len(data) - block_size --- (B)\n",
    "        # So, this is first argument passed to the random integer (randint) function\n",
    "        # this argument basically tells the function, hey find the POSITION of \n",
    "        # the character data between zero and this number. \n",
    "        # Note how it is total length - block_size, imagine if the full length of data was 38\n",
    "        # the sample of the POSITION of the data would not be more than 38-8 = 30\n",
    "        # this is because in the subsequent steps we will be extracting the value of the 30th item \n",
    "        # and the 31st item and the 32nd item all the way to the 8th item \n",
    "        # because we have a batch_size of 8. suppose we didn't have the -block_size part in there\n",
    "        # we could then randomly pick 35 or 36 or even 38, and that would screw up the\n",
    "        # subsequent step, where it would seek the next 8 values, but there wouldn't be the full 8\n",
    "        # and it would error out.\n",
    "        # \n",
    "        #\n",
    "        # so to bring it together (B) identifies the random position of anywhere in the data set.\n",
    "        # and (A) selects four such POSITIONS and return those back out and stores it in ix.\n",
    "        \n",
    "        \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "        # so what that does is it takes the 4 positions identifies in ix earlier, \n",
    "        # pulls the next block_size(8) and stack them up as ROWs in the tensor, \n",
    "        # so it would be an 4x8 (rows x columns) tensor\n",
    "    \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "        # it shows the stack x by an offset of 1..\n",
    "    return x,y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c129f07",
   "metadata": {},
   "source": [
    "So, I want to talk about why we printed the \"y\". Well this is going to be the loss function for our neural network.\n",
    "take an example of the first row printed:\n",
    "\n",
    "- x -> [55, 68, 23, 84,  1, 33, 64,  1]\n",
    "- y -> [68, 23, 84,  1, 33, 64,  1, 68]\n",
    "\n",
    "- if the input is 55, then the desired output is 68 (value in y in the SAME index as the x[])\n",
    "- if the input is 55, 68 then the desired output is 23 (value in y in the SAME index as the max value of x[])\n",
    "\n",
    "\n",
    "this loss function is applied on a neural network all the way in the end to measure what the NN spits out against what should have been the correct answer, and the delta between the NN output and the actual is the \"error\". This \"error\" is backpropagated through the neural network layers to adjust their weights so that the error is minimized.\n",
    "\n",
    "here is more details on the [loss function](https://www.analyticsvidhya.com/blog/2022/06/understanding-loss-function-in-deep-learning/) in ML\n",
    "\n",
    "\n",
    "Coming back to the above example, we have 32 values in x, and 32 values in y (desired targets). essentially we have 32 relationships stored in x and y\n",
    "\n",
    "repeating what we said earlier:\n",
    "\n",
    "- if the input is 55, then the desired output is 68 (value in y in the SAME index as the x[])\n",
    "- if the input is 55, 68 then the desired output is 23 (value in y in the SAME index as the max value of x[])\n",
    "- and so on.\n",
    "\n",
    "So this tensor below:\n",
    "xb tensor([[55, 68, 23, 84,  1, 33, 64,  1],\n",
    "        [69, 69,  1, 53, 68, 71, 55, 62],\n",
    "        [55,  1, 65, 64, 55,  1, 73, 59],\n",
    "        [75, 65, 71, 68,  1,  0, 66, 55]])\n",
    "        \n",
    "will feed into the transformer, and the transformer will simultaneously process these batches. and then look up the correct integers to predict in the same positions in the other tensor:\n",
    "\n",
    "\n",
    "yb tensor([[68, 23, 84,  1, 33, 64,  1, 68],\n",
    "        [69,  1, 53, 68, 71, 55, 62,  1],\n",
    "        [ 1, 65, 64, 55,  1, 73, 59, 70],\n",
    "        [65, 71, 68,  1,  0, 66, 55, 68]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df644b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:41:13.744220Z",
     "start_time": "2024-01-17T15:41:13.740218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the vocabulary: 87\n",
      "the actual vocabulary: \n",
      " !'()*+,-.0123459:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWYZ_abcdefghijklmnopqrstuvwxyz|~©»é—‘’“”\n"
     ]
    }
   ],
   "source": [
    "# checkpoint in the YT video. We are at 22:30\n",
    "# to recap:\n",
    "print(\"the length of the vocabulary: {}\".format(vocab_size))\n",
    "print(\"the actual vocabulary: {}\".format(''.join(chars)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82d2a0",
   "metadata": {},
   "source": [
    "#### Building a simple bigram language model\n",
    "\n",
    "Andrej covers the bigram model in depth in the first twenty minutes of [this](https://www.youtube.com/watch?v=PaCmpygFfXo) video. He took a list of human names and made the model create artificial names based on the how the characters in each name of the former followed each other.  \n",
    "\n",
    "The essential summary is:\n",
    "\n",
    "*A bigram language model is a type of statistical language model that predicts the probability of a word in a sequence based on the previous word. It considers pairs of consecutive words (bigrams) and estimates the likelihood of encountering a specific word given the preceding word in a text or sentence.*\n",
    "\n",
    "![bigram](https://d33wubrfki0l68.cloudfront.net/d851cde48c4f2b7be4c1433aa1a5538cb77e2aee/232ee/images/introduction-n-gram-language-models_files/ngram-language-model-explained-with-examples.png)B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd4685",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-17T15:40:24.703Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint in the YT video. We are at 22:30\n",
    "# to recap:\n",
    "print(\"the length of the vocabulary: {}\".format(vocab_size))\n",
    "print(\"the actual vocabulary: {}\".format(''.join(chars)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d6b49",
   "metadata": {},
   "source": [
    "#### nn.embeddings\n",
    "\n",
    "now before we dive into the next code, let's take a quick detour and understand pytorch embeddings. this one stumped me for a while.\n",
    "\n",
    "PS a good resource where I learnt this was Jeff Heaton's excellent [video](https://www.youtube.com/watch?v=e6kcs9Uj_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "341eb6bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T16:34:14.121433Z",
     "start_time": "2024-01-17T16:34:14.111503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3367,  0.1288],\n",
       "        [ 0.2345,  0.2303],\n",
       "        [-1.1229, -0.1863]], requires_grad=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "x = nn.Embedding(3,2)\n",
    "\n",
    "# so embeddings are just lookup tables, and what we did with the line above is\n",
    "# we create a tensor (aka a matrix of numbers) of 3x2 matrix and filled it with\n",
    "# a bunch of random numbers. to print this tensor out we can use:\n",
    "x.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "964503af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T16:40:03.498924Z",
     "start_time": "2024-01-17T16:40:03.491426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3367, 0.1288]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now if we want to look up certain values from this \"embedding\" lookup table\n",
    "# , we can call it like so:\n",
    "y = torch.tensor([0]) # we have to look up the embedding using a tensor, \n",
    "                        # not just any old integer will do\n",
    "x(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9030829c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T16:58:48.035048Z",
     "start_time": "2024-01-17T16:58:48.030179Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can see that we looked up the first row specified in y, aka row \"0\" \n",
    "# (since python arrays are zero-based) and printed that out. \n",
    "# this is useful to take any entity, e.g. words and REPRESENT them in \n",
    "# n-dimensions numerical vectors.\n",
    "# cow = [0.2323, 0.343434, 0.434343] \n",
    "# this allows us to see how close the word is to another word semantically like\n",
    "# animal = [0.3, 0.35, 0.48] vs far apart from another word like:\n",
    "# car = [0.8, 0.1, 0.03]\n",
    "# you can see that the 3 numbers (aka the vector) representing animal are closer to\n",
    "# those numbers representing cow, vs car. you can use things like mean squared\n",
    "# error to find out the single number which represents the closeness or \n",
    "# far-apart-ness for these numbers!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b7c871a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:00:27.381388Z",
     "start_time": "2024-01-17T17:00:27.372489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3367,  0.1288],\n",
      "        [ 0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important note: the size of the matrix that comes out of embedding is the\n",
    "# SAME as what went in. in the above example we sent it a size of 1 (when we\n",
    "# sent it [0]) and we got out a size of 1 [0.3367,0.1288]. its just that now\n",
    "# each array has a depth of 2 because we had made the embedding using a size of 2 each\n",
    "# like so: nn.Embedding(3,2)\n",
    "# heres a slightly more involved example:\n",
    "\n",
    "z = torch.tensor([0,1])\n",
    "print(x.weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "03171978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:01:39.295026Z",
     "start_time": "2024-01-17T17:01:39.287716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "tensor([[0.3367, 0.1288],\n",
      "        [0.2345, 0.2303]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# if you scroll up the 0th index in the embedding is represented by [0.3367, 0.1288]\n",
    "# and the 1th index is [0.2345, 0.2303]\n",
    "\n",
    "print(z)\n",
    "print(x(z))\n",
    "\n",
    "\n",
    "# so with that, let's resume the Andrej video, we are at 23:07 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "46b176d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:07:13.805634Z",
     "start_time": "2024-01-17T17:07:13.798091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1],\n",
      "        [1, 0, 0]])\n",
      "tensor([[[0.3367, 0.1288],\n",
      "         [0.2345, 0.2303],\n",
      "         [0.2345, 0.2303]],\n",
      "\n",
      "        [[0.2345, 0.2303],\n",
      "         [0.3367, 0.1288],\n",
      "         [0.3367, 0.1288]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# above, 0 is represented by a vector of 2 numbers [0.3367, 0.1288], \n",
    "# and similarly 1 is represented also a vector of 2 numbers [0.2345, 0.2303]\n",
    "\n",
    "# now, if we passed it a tensor of more complexity see how that is represented \n",
    "# by the embeddings...\n",
    "t1 = torch.tensor([[0,1,1], [1,0,0]])\n",
    "print(t1)\n",
    "print(x(t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "da98bae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:08:43.149370Z",
     "start_time": "2024-01-17T17:08:43.144239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# now \n",
    "\n",
    "# 0th row of the embedding, then the 1th row of the embedding, and then again\n",
    "# the 1th row of the embedding.\n",
    "# the size is:\n",
    "print(x(t1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which means a tensor of TWO, 3x2 matrices, and each of these two matrices is\n",
    "# a representing of each value (eg [0, 1, 1]) within the input tensor, \n",
    "# within each 3x2 matrix, each row represents the (2 dimensional vector representation)\n",
    "# of each of the numbers in the input, e.g. 0, or 1, or 1.. so\n",
    "# 0 is represented by [0.3367, 0.1288], the next\n",
    "# 1 is represtned by [0.2345, 0.2303], and the next\n",
    "# 1 is represtned by [0.2345, 0.2303]\n",
    "# and therefore [0, 1, 1] is represented by:\n",
    "# [0.3367, 0.1288],\n",
    "# [0.2345, 0.2303],\n",
    "# [0.2345, 0.2303]\n",
    "# rinse and repeat for the next input [1, 0, 0]\n",
    "\n",
    "# why am I going into this depth, because things are about to get crazy in \n",
    "# the n dimensions when we call the nn.Embedding method in the next stage.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3eb38568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T18:51:23.006902Z",
     "start_time": "2024-01-17T18:51:22.979711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 87])\n",
      "tensor(4.7719, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Cug__>Om-B»n“MFxP1iw3a*U(qe'fwTqfEpWUc©Rd~e—DVc5:urRotUHDuO-!Ag_ rOYCuR.Wx5\n",
      "G0tf3”@b>*3c=cWbql4xhoCd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    # the BigramLanguageModel will inherit from nn.Module class\n",
    "    # so it would be a subclass of nn.Module\n",
    "    \n",
    "    #initializing method:\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # to recap: \n",
    "        # the length of the vocabulary: 87\n",
    "        # the actual vocabulary: \n",
    "        # !'()*+,-.0123459:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWYZ_abcdefghijklmnopqrstuvwxyz|~©»é—‘’“”\n",
    "        # nn.Embedding has been discussed above (aka its a lookup table), so\n",
    "        # what we're doing is that we are representing each character in the \n",
    "        # vocabulary above into an 87 dimension vector\n",
    "        \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # takes the inputs and target tensors, input renamed as idx\n",
    "        # recall that the input was a stacked tensor of batch (rows) of 4\n",
    "        # with 8 tokens (here, that is same as characters) across, eg\n",
    "        # tensor([\n",
    "        # [55, 68, 23, 84,  1, 33, 64,  1],\n",
    "        # [69, 69,  1, 53, 68, 71, 55, 62],\n",
    "        # [55,  1, 65, 64, 55,  1, 73, 59],\n",
    "        # [75, 65, 71, 68,  1,  0, 66, 55]])\n",
    "        \n",
    "       # idx and targets are both (B,T)B tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # so recall the long explanation of the pytorch embeddings above.\n",
    "        # each of the rows in the input, aka the encoded chunk we are going to use to train\n",
    "        # eg [55, 68, 23, 84,  1, 33, 64,  1]\n",
    "        # is taken and then each of the numbers (eg 55) is going to be blown into its \n",
    "        # representation of an 87 number VECTOR, which is the 55th row\n",
    "        # in the embedding table. \n",
    "        # so we shoved in 4x8 and should get a 4x8x87 matrix as an output.\n",
    "        # we call this Batch x Time x Channel. The batch is 4, the Time is 8\n",
    "        # and the channel is 87.\n",
    "        \n",
    "        # a peek forward logits is going to essentially represent the scores\n",
    "        # of the next character in the sequence, and this is where the target\n",
    "        # tensor will come in.. (24:12)\n",
    "        \n",
    "        # basically if i'm character 58, (say \"y\") I know just by being \"y\"\n",
    "        # what are the probabilities of what character will follow vs not.\n",
    "        # say \"a\" has a higher probability of following \"y\" vs \"z\". (in fact, \n",
    "        # let's assume the probability of z following y is 0! that would mean\n",
    "        # for the 58th index in the Embedding (aka for y) , the 59th value (or \n",
    "        # the number representing the character z) would be 0 and say the\n",
    "        # 7th value (or number representing a) could be 0.8\n",
    "        \n",
    "       \n",
    "        # next step is is evaluate the loss function\n",
    "        \n",
    "    \n",
    "        #loss = F.cross_entropy(logits, targets) (a) commented bc it fails,\n",
    "                            # we need to do some transforms\n",
    "        \n",
    "        # this means , we have the identity of the next character in 'targets'\n",
    "        # so how well are we predicting the next character in the 'logits'\n",
    "        \n",
    "        # intuitively, if the loss is low (meaning accuracy is high), the\n",
    "        # correct next number should be very high (such as the 7th value example above)\n",
    "        # and every other value would be a low number.\n",
    "        \n",
    "        # the magic of using tensors with these crazy n dimensional arrays is that you\n",
    "        # are doing a bunch of mathametical operations in parallel together \n",
    "        # which is supporting very nicely in GPUs <-- this has enabled the massive\n",
    "        # scaling and speedup of pre-training an LLM which takes 12 days with GPUs\n",
    "        # vs it would have taken 12 centuries with CPUs!\n",
    "        \n",
    "        # the reason (a) was commented was because if you look carefully at what\n",
    "        # cross entropy is expecting \n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "        # it is expecting not a torch.Size([4, 8, 87]) which is what we will are passing\n",
    "        # it rn. Instead it is expecting 87 (channels) as the 2nd dimension, not the 3rd!\n",
    "               \n",
    "            \n",
    "        # but to predict by the \"generate() function\"\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "\n",
    "            # \n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # C or channels needs to be 2nd dimension\n",
    "            # so we are stretching out the array so its two dimensional and conforms to\n",
    "            # cross_entropy(). \n",
    "\n",
    "            #doing the same to targets:\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # and now cross_entropy should work:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # at this point \n",
    "            # print(logits.shape) is torch.Size([32, 87])\n",
    "            # print(loss) is  tensor(4.7719, grad_fn=<NllLossBackward0>)\n",
    "            # an ideal negative loss likelihood unitary number should be\n",
    "            # -ln(1/87) ==> 4.4659\n",
    "\n",
    "            # this means initial predictions are not super diffuse and have a little\n",
    "            # entropy\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        # the job if generate() is to take in the batches, B,T and generate\n",
    "        #(B,T+1), (B,T+2) and so on in the time dimension.\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # the self(idx) will call the forward function (hmm! must be a pytorch thing)\n",
    "            # \n",
    "            \n",
    "            #focus only on the last step, i.e what comes after the last character\n",
    "            logits = logits[:, -1, :] # becomes shape (B,C) or (4,87), the last column\n",
    "            \n",
    "            #now, apply softmax to get probabilities\n",
    "            # https://en.wikipedia.org/wiki/Softmax_function\n",
    "            #converts a vector of K real numbers into a probability distribution of \n",
    "            # K possible outcomes. It is a generalization of the logistic function\n",
    "            # to multiple dimensions \n",
    "            # The softmax function is often used as the last activation function \n",
    "            # of a neural network to normalize the output of a network \n",
    "            # to a probability distribution over predicted output classes\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (B,1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            # so we are concatenating what was predicted prior and written on top\n",
    "            # of idx, and generate as many as max_new_tokens has specified.\n",
    "            \n",
    "            \n",
    "        return idx\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size) \n",
    "logits, loss = m(xb,yb)\n",
    "# this line calls the __init__ and creates m, an object of the BigramLanguageModel \n",
    "# class\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "storage_idx = torch.zeros((1,1), dtype=torch.long)\n",
    "# where we will store the output generated tokens\n",
    "\n",
    "# generating a 100 tokens\n",
    "print(decode(m.generate(storage_idx, \n",
    "                        max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f996cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above prints out garbage because this is a totally random model.\n",
    "# we are not back propagating and reducing loss so it runs through\n",
    "# using random numbers.\n",
    "\n",
    "# also we are sending the model the whole length of data just to predict the next \n",
    "# char, e.g. to predict the last U we are sending in the full history\n",
    "# Cug__>Om-B»n“MFxP1iw3a*U(qe'fwTqfEpW \n",
    "# as well. in bigram the history doesn't matter but in the actual\n",
    "# transformer model it will, so we want to keep this code as is instead \n",
    "# of sending a pruned single char input.\n",
    "\n",
    "# stopping at 34:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
