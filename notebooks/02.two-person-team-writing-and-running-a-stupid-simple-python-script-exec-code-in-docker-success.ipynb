{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3127e27e",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Attempt-02.-AI-Product-Team---ML-Ops-app\" data-toc-modified-id=\"Attempt-02.-AI-Product-Team---ML-Ops-app-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Attempt 02. AI Product Team - ML Ops app</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Post-Run-Summary:\" data-toc-modified-id=\"Post-Run-Summary:-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span><em>Post-Run Summary:</em></a></span></li><li><span><a href=\"#first-let's-pull-down-the-libraries-we-need\" data-toc-modified-id=\"first-let's-pull-down-the-libraries-we-need-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>first let's pull down the libraries we need</a></span></li><li><span><a href=\"#start-off-agents-locally\" data-toc-modified-id=\"start-off-agents-locally-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>start off agents locally</a></span></li><li><span><a href=\"#post-attempt-update-around-running-local-LLMs-behind-API.\" data-toc-modified-id=\"post-attempt-update-around-running-local-LLMs-behind-API.-1.0.4\"><span class=\"toc-item-num\">1.0.4&nbsp;&nbsp;</span>post-attempt update around running local LLMs behind API.</a></span></li><li><span><a href=\"#set-up-the-team!\" data-toc-modified-id=\"set-up-the-team!-1.0.5\"><span class=\"toc-item-num\">1.0.5&nbsp;&nbsp;</span>set up the team!</a></span></li><li><span><a href=\"#Set-the-team-to-work!\" data-toc-modified-id=\"Set-the-team-to-work!-1.0.6\"><span class=\"toc-item-num\">1.0.6&nbsp;&nbsp;</span>Set the team to work!</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c0233",
   "metadata": {
    "cell_style": "center",
    "solution": "hidden"
   },
   "source": [
    "# Attempt 02. AI Product Team - ML Ops app\n",
    "\n",
    "Setting up a product team which ~~does research for me on the latest academic research done on ML Ops and presents a summary for me.~~ does a stupid piece of python code which counts numbers. lets try just getting python to work first and we'll learn and build from here.\n",
    "\n",
    "Also, this time i'm going to try to get the AI code to execute code in a docker container instead of trying it within notebook and bombing out \n",
    "\n",
    "### _Post-Run Summary:_\n",
    "While this attempt failed, I am still saving and keeping it, because the chat between the product manager and software engineering was very interesting. \n",
    "\n",
    "I also strugged with using localAI to host my LLM model. It just launched the service too fast, without taking any time to load the model in memory. I did not have this problem with LM Studio hosting it, but that one took a little longer in hosting it. Check out section 1.03 and 1.04 below for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08820245",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "### first let's pull down the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1dd07a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T18:17:15.653850Z",
     "start_time": "2023-12-29T18:17:14.552090Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "outputs": [],
   "source": [
    "# import all relevant libraries\n",
    "import autogen\n",
    "import docker\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11118197",
   "metadata": {},
   "source": [
    "### start off agents locally\n",
    "\n",
    "_Update: Read this section to the end, the localAI method didnt work in this run_\n",
    "This step is done in shell. You have to download the required LLMs (I used ollama, but you can use LM Studio too) and then put them behind an open AI compatible serve (I used local-ai). Then you have to run them.. For this experiment I'm attempting to  two agents like so:\n",
    "\n",
    "```./local-ai --models-path=/Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/deepseek-coder-6.7B-instruct-Gsts/registry.ollama.dress=\":3003\"\n",
    "-address=\":3003\"\n",
    "6:37PM DBG no galleries to load\n",
    "6:37PM INF Starting LocalAI using 4 threads, with models path: /Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/deepseek-coder-6.7B-instruct-GGUF\n",
    "6:37PM INF LocalAI version: v2.2.0-19-gc45f581 (c45f581c47b2132257567898284e2d30ed9fa0cf)\n",
    "6:37PM DBG Extracting backend assets files to /tmp/localai/backend_data\n",
    "\n",
    " ┌───────────────────────────────────────────────────┐\n",
    " │                   Fiber v2.50.0                   │\n",
    " │               http://127.0.0.1:3003               │\n",
    " │       (bound on host 0.0.0.0 and port 3003)       │\n",
    " │                                                   │\n",
    " │ Handlers ............ 74  Processes ........... 1 │\n",
    " │ Prefork ....... Disabled  PID ............. 38141 │\n",
    " └───────────────────────────────────────────────────┘\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "./local-ai --models-path=/Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF --debug=true --address \":3002\n",
    "istry.ollama.ai/TheBloke/Orca-2-13B-GGUF --debug=true --address \":3002\"\n",
    "6:33PM DBG no galleries to load\n",
    "6:33PM INF Starting LocalAI using 4 threads, with models path: /Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF\n",
    "6:33PM INF LocalAI version: v2.2.0-19-gc45f581 (c45f581c47b2132257567898284e2d30ed9fa0cf)\n",
    "6:33PM DBG Extracting backend assets files to /tmp/localai/backend_data\n",
    "\n",
    " ┌───────────────────────────────────────────────────┐\n",
    " │                   Fiber v2.50.0                   │\n",
    " │               http://127.0.0.1:3002               │\n",
    " │       (bound on host 0.0.0.0 and port 3002)       │\n",
    " │                                                   │\n",
    " │ Handlers ............ 74  Processes ........... 1 │\n",
    " │ Prefork ....... Disabled  PID ............. 37748 │\n",
    " └───────────────────────────────────────────────────┘\n",
    "\n",
    "[127.0.0.1]:52748 404 - GET /\n",
    "[127.0.0.1]:52748 404 - GET /favicon.ico\n",
    "[127.0.0.1]:52748 404 - GET /v1\n",
    "```\n",
    "\n",
    "### post-attempt update around running local LLMs behind API.\n",
    "\n",
    "When I ran the API using localAI, I kept seeing it trying to run /gpt-4 behind\n",
    "\n",
    "```\n",
    "tcm LocalAI$ ./local-ai --models-path=/Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF --debug=true --address \":3002\"\n",
    "11:59PM DBG no galleries to load\n",
    "11:59PM INF Starting LocalAI using 4 threads, with models path: /Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF\n",
    "11:59PM INF LocalAI version: v2.2.0-19-gc45f581 (c45f581c47b2132257567898284e2d30ed9fa0cf)\n",
    "11:59PM DBG Extracting backend assets files to /tmp/localai/backend_data\n",
    "\n",
    " ┌───────────────────────────────────────────────────┐\n",
    " │                   Fiber v2.50.0                   │\n",
    " │               http://127.0.0.1:3002               │\n",
    " │       (bound on host 0.0.0.0 and port 3002)       │\n",
    " │                                                   │\n",
    " │ Handlers ............ 74  Processes ........... 1 │\n",
    " │ Prefork ....... Disabled  PID ............. 41554 │\n",
    " └───────────────────────────────────────────────────┘\n",
    "\n",
    "zzz12:57AM DBG Request received:\n",
    "12:57AM DBG Configuration read: &{PredictionOptions:{Model:gpt-4 Language: N:0 TopP:0.7 TopK:80 Temperature:0.9 Maxtokens:512 Echo:false Batch:0 F16:false IgnoreEOS:false RepeatPenalty:0 Keep:0 MirostatETA:0 MirostatTAU:0 Mirostat:0 FrequencyPenalty:0 TFZ:0 TypicalP:0 Seed:0 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name: F16:false Threads:4 Debug:true Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat: ChatMessage: Completion: Edit: Functions:} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0 MirostatTAU:0 Mirostat:0 NGPULayers:0 MMap:false MMlock:false LowVRAM:false Grammar: StopWords:[] Cutstrings:[] TrimSpace:[] ContextSize:512 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: MMProj: RopeScaling: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false}\n",
    "12:57AM DBG Parameters: &{PredictionOptions:{Model:gpt-4 Language: N:0 TopP:0.7 TopK:80 Temperature:0.9 Maxtokens:512 Echo:false Batch:0 F16:false IgnoreEOS:false RepeatPenalty:0 Keep:0 MirostatETA:0 MirostatTAU:0 Mirostat:0 FrequencyPenalty:0 TFZ:0 TypicalP:0 Seed:0 NegativePrompt: RopeFreqBase:0 RopeFreqScale:0 NegativePromptScale:0 UseFastTokenizer:false ClipSkip:0 Tokenizer:} Name: F16:false Threads:4 Debug:true Roles:map[] Embeddings:false Backend: TemplateConfig:{Chat: ChatMessage: Completion: Edit: Functions:} PromptStrings:[] InputStrings:[] InputToken:[] functionCallString: functionCallNameString: FunctionsConfig:{DisableNoAction:false NoActionFunctionName: NoActionDescriptionName:} FeatureFlag:map[] LLMConfig:{SystemPrompt: TensorSplit: MainGPU: RMSNormEps:0 NGQA:0 PromptCachePath: PromptCacheAll:false PromptCacheRO:false MirostatETA:0 MirostatTAU:0 Mirostat:0 NGPULayers:0 MMap:false MMlock:false LowVRAM:false Grammar: StopWords:[] Cutstrings:[] TrimSpace:[] ContextSize:512 NUMA:false LoraAdapter: LoraBase: LoraScale:0 NoMulMatQ:false DraftModel: NDraft:0 Quantization: MMProj: RopeScaling: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0} AutoGPTQ:{ModelBaseName: Device: Triton:false UseFastTokenizer:false} Diffusers:{CUDA:false PipelineType: SchedulerType: EnableParameters: CFGScale:0 IMG2IMG:false ClipSkip:0 ClipModel: ClipSubFolder: ControlNet:} Step:0 GRPC:{Attempts:0 AttemptsSleepTime:0} VallE:{AudioPath:} CUDA:false}\n",
    "12:57AM DBG Prompt (before templating): You are a coder specializing in Python\n",
    "write a program which output numbers 1 to a 100\n",
    "12:57AM DBG Prompt (after templating): You are a coder specializing in Python\n",
    "write a program which output numbers 1 to a 100\n",
    "12:57AM DBG Loading model 'gpt-4' greedly from all the available backends: llama-cpp, llama-ggml, llama, gpt4all, gptneox, bert-embeddings, falcon-ggml, gptj, gpt2, dolly, mpt, replit, starcoder, rwkv, whisper, stablediffusion, tinydream, piper\n",
    "12:57AM DBG [llama-cpp] Attempting to load\n",
    "12:57AM INF Loading model 'gpt-4' with backend llama-cpp\n",
    "12:57AM DBG Loading model in memory from file: /Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF/gpt-4\n",
    "12:57AM DBG Loading Model gpt-4 with gRPC (file: /Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF/gpt-4) (backend: llama-cpp): {backendString:llama-cpp model:gpt-4 threads:4 assetDir:/tmp/localai/backend_data context:{emptyCtx:{}} gRPCOptions:0x1400021e5a0 externalBackends:map[] grpcAttempts:20 grpcAttemptsDelay:2 singleActiveBackend:false parallelRequests:false}\n",
    "12:57AM DBG Loading GRPC Process: /tmp/localai/backend_data/backend-assets/grpc/llama-cpp\n",
    "12:57AM DBG GRPC Service for gpt-4 will be running at: '127.0.0.1:54660'\n",
    "12:57AM DBG GRPC Service state dir: /var/folders/z9/jt4cn2td0gn40p7sh34cbzg80000gn/T/go-processmanager1776259398\n",
    "12:57AM DBG GRPC Service Started\n",
    "rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:54660: connect: connection refused\"\n",
    "12:57AM DBG GRPC(gpt-4-127.0.0.1:54660): stdout Server listening on 127.0.0.1:54660\n",
    "12:58AM DBG GRPC Service Ready\n",
    "12:58AM DBG GRPC: Loading model with options: {state:{NoUnkeyedLiterals:{} DoNotCompare:[] DoNotCopy:[] atomicMessageInfo:<nil>} sizeCache:0 unknownFields:[] Model:gpt-4 ContextSize:512 Seed:0 NBatch:512 F16Memory:false MLock:false MMap:false VocabOnly:false LowVRAM:false Embeddings:false NUMA:false NGPULayers:0 MainGPU: TensorSplit: Threads:4 LibrarySearchPath: RopeFreqBase:0 RopeFreqScale:0 RMSNormEps:0 NGQA:0 ModelFile:/Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF/gpt-4 Device: UseTriton:false ModelBaseName: UseFastTokenizer:false PipelineType: SchedulerType: CUDA:false CFGScale:0 IMG2IMG:false CLIPModel: CLIPSubfolder: CLIPSkip:0 ControlNet: Tokenizer: LoraBase: LoraAdapter: LoraScale:0 NoMulMatQ:false DraftModel: AudioPath: Quantization: MMProj: RopeScaling: YarnExtFactor:0 YarnAttnFactor:0 YarnBetaFast:0 YarnBetaSlow:0}\n",
    "12:58AM DBG GRPC(gpt-4-127.0.0.1:54660): stderr error loading model: failed to open /Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF/gpt-4: No such file or directory\n",
    "12:58AM DBG GRPC(gpt-4-127.0.0.1:54660): stderr llama_load_model_from_file: failed to load model\n",
    "12:58AM DBG GRPC(gpt-4-127.0.0.1:54660): stderr llama_init_from_gpt_params: error: failed to load model '/Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF/gpt-4'\n",
    "12:58AM DBG GRPC(gpt-4-127.0.0.1:54660): stdout {\"timestamp\":1703833080,\"level\":\"ERROR\",\"function\":\"load_model\",\"line\":589,\"message\":\"unable to load model\",\"model\":\"/Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF/gpt-4\"}\n",
    "12:58AM DBG [llama-cpp] Fails: could not load model: rpc error: code = Canceled desc =\n",
    "12:58AM DBG [llama-ggml] Attempting to load\n",
    "12:58AM INF Loading model 'gpt-4' with backend llama-ggml\n",
    "12:58AM DBG Loading model in memory from file: /Users/knail1/.ollama/models/manifests/registry.ollama.ai/TheBloke/Orca-2-13B-GGUF/gpt-4\n",
    "```\n",
    "\n",
    "After that, it just kept repeating the above message while my code in the notebook (below) kept hanging, until it bombed out in the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69712060",
   "metadata": {},
   "source": [
    "\n",
    "I switched to LM studio after this, and it ran perfectly the first time. here are the logs from that:\n",
    "\n",
    "```\n",
    "[2023-12-29 05:27:32.481] [INFO] [LM STUDIO SERVER] Verbose server logs are ENABLED\n",
    "[2023-12-29 05:27:32.483] [INFO] [LM STUDIO SERVER] Success! HTTP server listening on port 3010\n",
    "[2023-12-29 05:27:32.483] [INFO] [LM STUDIO SERVER] Supported endpoints:\n",
    "[2023-12-29 05:27:32.483] [INFO] [LM STUDIO SERVER] ->\tGET  http://localhost:3010/v1/models\n",
    "[2023-12-29 05:27:32.484] [INFO] [LM STUDIO SERVER] ->\tPOST http://localhost:3010/v1/chat/completions\n",
    "[2023-12-29 05:27:32.484] [INFO] [LM STUDIO SERVER] ->\tPOST http://localhost:3010/v1/completions\n",
    "[2023-12-29 05:27:32.484] [INFO] [LM STUDIO SERVER] Logs are saved into /tmp/lmstudio-server-log.txt\n",
    "[2023-12-29 05:27:32.484] [INFO] [LM STUDIO SERVER] Setting context overflow policy to: Rolling Window\n",
    "[2023-12-29 05:30:09.248] [INFO] [LM STUDIO SERVER] Processing queued request...\n",
    "[2023-12-29 05:30:09.250] [INFO] Received POST request to /v1/chat/completions with body: {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": \"You are a coder specializing in Python\",\n",
    "      \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "      \"content\": \"write a program which output numbers 1 to a 100\",\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"model\": \"gpt-4\",\n",
    "  \"stream\": false\n",
    "}\n",
    "[2023-12-29 05:30:09.251] [INFO] [LM STUDIO SERVER] Context Overflow Policy is: Rolling Window\n",
    "[2023-12-29 05:30:09.252] [INFO] Provided inference configuration: {\n",
    "  \"n_threads\": 4,\n",
    "  \"n_predict\": -1,\n",
    "  \"top_k\": 40,\n",
    "  \"min_p\": 0.05,\n",
    "  \"top_p\": 0.95,\n",
    "  \"temp\": 0.8,\n",
    "  \"repeat_penalty\": 1.1,\n",
    "  \"input_prefix\": \"### Instruction:\\n\",\n",
    "  \"input_suffix\": \"\\n### Response:\\n\",\n",
    "  \"antiprompt\": [\n",
    "    \"### Instruction:\"\n",
    "  ],\n",
    "  \"pre_prompt\": \"You are a coder specializing in Python\",\n",
    "  \"pre_prompt_suffix\": \"\\n\",\n",
    "  \"pre_prompt_prefix\": \"\",\n",
    "  \"seed\": -1,\n",
    "  \"tfs_z\": 1,\n",
    "  \"typical_p\": 1,\n",
    "  \"repeat_last_n\": 64,\n",
    "  \"frequency_penalty\": 0,\n",
    "  \"presence_penalty\": 0,\n",
    "  \"n_keep\": 0,\n",
    "  \"logit_bias\": {},\n",
    "  \"mirostat\": 0,\n",
    "  \"mirostat_tau\": 5,\n",
    "  \"mirostat_eta\": 0.1,\n",
    "  \"memory_f16\": true,\n",
    "  \"multiline_input\": false,\n",
    "  \"penalize_nl\": true\n",
    "}\n",
    "[2023-12-29 05:30:09.253] [INFO] [LM STUDIO SERVER] Last message: { role: 'user', content: 'write a program which output numbers 1 to a 100' } (total messages = 2)\n",
    "[2023-12-29 05:30:10.314] [INFO] [LM STUDIO SERVER] Accumulating tokens ... (stream = false)\n",
    "[2023-12-29 05:30:10.314] [INFO] Accumulated 1 tokens: Here\n",
    "[2023-12-29 05:30:10.425] [INFO] Accumulated 2 tokens: Here is\n",
    "[2023-12-29 05:30:10.537] [INFO] Accumulated 3 tokens: Here is a\n",
    "[2023-12-29 05:30:10.647] [INFO] Accumulated 4 tokens: Here is a possible\n",
    "[2023-12-29 05:30:10.756] [INFO] Accumulated 5 tokens: Here is a possible solution\n",
    "[2023-12-29 05:30:10.867] [INFO] Accumulated 6 tokens: Here is a possible solution:\n",
    "[2023-12-29 05:30:10.978] [INFO] Accumulated 7 tokens: Here is a possible solution:\\n\n",
    "[2023-12-29 05:30:11.088] [INFO] Accumulated 8 tokens: Here is a possible solution:\\n\\n\n",
    "[2023-12-29 05:30:11.198] [INFO] Accumulated 9 tokens: Here is a possible solution:\\n\\nfor\n",
    "[2023-12-29 05:30:11.308] [INFO] Accumulated 10 tokens: Here is a possible solution:\\n\\nfor i\n",
    "[2023-12-29 05:30:11.418] [INFO] Accumulated 11 tokens: Here is a possible solution:\\n\\nfor i in\n",
    "[2023-12-29 05:30:11.528] [INFO] Accumulated 12 tokens: Here is a possible solution:\\n\\nfor i in range\n",
    "[2023-12-29 05:30:11.638] [INFO] Accumulated 13 tokens: Here is a possible solution:\\n\\nfor i in range(\n",
    "[2023-12-29 05:30:11.749] [INFO] Accumulated 14 tokens: Here is a possible solution:\\n\\nfor i in range(1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44b426",
   "metadata": {
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "### set up the team!\n",
    "\n",
    "We will connect to the locally spun up APIs for the two local LLMs and give them the product manager and sw engineer personas.\n",
    "We will use this [link](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_web_info.ipynb) which shows how to use web info to analyze stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a4cb22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T18:17:15.694226Z",
     "start_time": "2023-12-29T18:17:15.655090Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "solution": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config_list': [{'base_url': 'http://127.0.0.1:3002/v1', 'api_key': 'NULL'}]}=\n",
      "{'config_list': [{'base_url': 'http://127.0.0.1:3003/v1', 'api_key': 'NULL'}]}=\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "# orca2_config_list = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": [\"orca2\"]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# deepseekcoder_config_list = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": [\"deepseekcoder\"]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#api_keys = [\"NULL\"]\n",
    "\n",
    "# orca2_config_list = autogen.get_config_list(\n",
    "#     api_keys,\n",
    "#     base_urls=\"https://localhost:3002/v1\",\n",
    "# )\n",
    "\n",
    "# deepseekcoder_config_list = [\n",
    "#     {\n",
    "#         #'model': 'deepseekcoder',\n",
    "#         \"api_base\": \"http://127.0.0.1:3003/v1\",\n",
    "#         \"api_key\": \"NULL\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# deepseekcoder_config_list = autogen.get_config_list(\n",
    "#     api_keys,\n",
    "#     base_urls=\"https://localhost:3003/v1\",\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "orca2_config_list = [{\n",
    "        \"base_url\": \"http://127.0.0.1:3002/v1\",\n",
    "        \"api_key\": \"NULL\"\n",
    "}]\n",
    "orca2_llm_config={\"config_list\": orca2_config_list}\n",
    "\n",
    "orca2_config_list_lmstudio = [{\n",
    "        \"base_url\": \"http://127.0.0.1:3010/v1\",\n",
    "        \"api_key\": \"NULL\"\n",
    "}]\n",
    "orca2_llm_config_lmstudio={\"config_list\": orca2_config_list_lmstudio}\n",
    "\n",
    "\n",
    "\n",
    "deepseekcoder_config_list = [{\n",
    "        \"base_url\": \"http://127.0.0.1:3003/v1\",\n",
    "        \"api_key\": \"NULL\"\n",
    "    }]\n",
    "deepseekcoder_llm_config={\"config_list\": deepseekcoder_config_list}\n",
    "\n",
    "\n",
    "print(f\"{orca2_llm_config}=\")\n",
    "print(f\"{deepseekcoder_llm_config}=\")\n",
    "\n",
    "\n",
    "# user_proxy = autogen.UserProxyAgent(\n",
    "#     name='mr_jeeves',\n",
    "#     system_message='Admin on behalf of the manager',\n",
    "#     code_execution_config=(\"last_n_messages\": 2,\n",
    "#                            \"work_dir\": \"groupchat\"),\n",
    "#     human_input_mode=\"TERMINATE\"\n",
    "        \n",
    "# )\n",
    "\n",
    "\n",
    "## I had to create a local directory ./web \n",
    "\n",
    "# create a UserProxyAgent instance named \"product_manager\"\n",
    "product_manager = autogen.UserProxyAgent(\n",
    "    name=\"product_manager\",\n",
    "    #system_message=\"product manager who does producty things\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\"work_dir\": \"web\", \"use_docker\": \"autogen-project:latest\" },\n",
    "    llm_config=orca2_llm_config_lmstudio, \n",
    "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\"\n",
    ")\n",
    "\n",
    "#create the AI software engineer\n",
    "sw_engineer= autogen.AssistantAgent(\n",
    "    name='sw_engineer',\n",
    "    system_message=\"You are a coder specializing in Python\",\n",
    "#    llm_config=deepseekcoder_llm_config\n",
    "        llm_config=orca2_llm_config_lmstudio\n",
    ")\n",
    "\n",
    "#create the AI product manager \n",
    "# pdm = autogen.AssistantAgent(\n",
    "#     name='product_manager',\n",
    "#     system_message=\"product manager who does producty things\",\n",
    "#     llm_config={\"config_list\": orca2_config_list, \n",
    "#                 \"seed\":42},\n",
    "# )\n",
    "\n",
    "#create RAG agent (researches data via local or online options)\n",
    "# later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26feca",
   "metadata": {},
   "source": [
    "### Set the team to work!\n",
    "\n",
    "Now we will give a simple instruction to the AI product manager who will then work with the software engineer to do the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d9a55",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-29T18:17:14.591Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "write a python program which output numbers 1 to a 100 and then run it, and save it in a file called output.txt\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "I'm sorry, I cannot run or save files for you. However, I can provide you with a Python code that outputs the numbers from 1 to 100. You can copy and paste this code into an online interpreter like repl.it or execute it in your local environment. Then, you can use the print() function to write the output to a file called output.txt. Here's the code:\n",
      "\n",
      "```python\n",
      "for i in range(1, 101):\n",
      "  print(i)\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "I'm glad the code worked for you. If you have any other questions or need further assistance, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "thank you\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "You're welcome. Have a great day!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "bye\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "Goodbye and take care!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "bye\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "Goodbye and take care!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "bye\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "Goodbye and take care!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "bye\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "Goodbye and take care!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "bye\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "Goodbye and take care!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "bye\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "Goodbye and take care!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "bye\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "Goodbye and take care!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mproduct_manager\u001b[0m (to sw_engineer):\n",
      "\n",
      "bye\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msw_engineer\u001b[0m (to product_manager):\n",
      "\n",
      "Goodbye and take care!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# the assistant receives a message from the user, which contains the task description\n",
    "\n",
    "#task = \"\"\"\n",
    "#Find me all the highest cited academic papers of 2022 and 2023 on arxiv about the topic of MLOps, extract their abstracts and save them in a pandas table with the name of the paper, the name of the first author, the number of citations, and the abstract\n",
    "#\"\"\"\n",
    "\n",
    "task = \"write a python program which output numbers 1 to a 100 and then run it, and save it in a file called output.txt\"\n",
    "product_manager.initiate_chat(\n",
    "    sw_engineer,\n",
    "    message=task\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe328f",
   "metadata": {},
   "source": [
    "Since the arxiv app was complex, it required some pre-setup for environment (installed python modules from bash shell and then running python executable) , I dumbed the script down to just a python script to count numbers..\n",
    "\n",
    "it didn't exactly do what I wanted (save the output in the disk) but it still ran it. what was funny was that the code completed a while back, but I had set set `max_consecutive_auto_reply=10` so the two AI bots kept saying dumb stuff to each other long after the code had completed.\n",
    "\n",
    "to make it interesting I ran this code in a docker container (search for use_docker above) , and I had docker running locally on my laptop. I did see it create a new image for this run.\n",
    "\n",
    "ok, what we're learning here is that before we run code, we need to have explicit instructions to set up environment. \n",
    "\n",
    "In the next run, I'll try to define my requirements much more explicitly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "185.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 120.852,
   "position": {
    "height": "142.852px",
    "left": "395px",
    "right": "20px",
    "top": "271px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
